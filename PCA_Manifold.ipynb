{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of intro_to_PCA_Manifold.ipynb","version":"0.3.2","provenance":[{"file_id":"1rE-7NniRftCRrGTFXUX6pqqE-BRsjpJO","timestamp":1557897594578}],"collapsed_sections":["z4AnSP-DrLFr","kZg1jyY5rLFs","snFkXeKSrLFw","w99a2X7CrLF3","L2rXBWfBrLF7","-7KunpfArLHT"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.4"},"kernelspec":{"display_name":"Python [conda env:tensorflow]","language":"python","name":"conda-env-tensorflow-py"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yg67lQ66rLFZ","slideshow":{"slide_type":"slide"}},"source":["## Principal Component Analysis (PCA)\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wIWvVEuJrLFb","slideshow":{"slide_type":"slide"}},"source":["### Learning Objectives\n","\n","- Describe what PCA does and what it is used for in data science.\n","- Practice computing PCA with sklearn.\n","- Interpret PCA results graphically."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DAVXuaHTrLFb","slideshow":{"slide_type":"slide"}},"source":["### Lesson Guide\n","- [Motivation](#motivation)\n","- [What is PCA?](#whatispca)\n","    - [Eigenvalues and Eigenvectors](#eigenpairs)\n","    - [Principal Components](#pcs)\n","- [Manual PCA Codealong](#manual-codealong)\n","    - [1. Basic EDA](#basic-eda)\n","    - [2. Subset and Normalize](#subset)\n","    - [3. Find the Correlation Matrix](#corr)\n","    - [4. Eignenvalues and Eigenvectors](#eigen)\n","    - [5. Explained Variance](#var)\n","    - [6. Projection Matrix W](#projection)\n","    - [7. Transformed Matrix Z](#transformed)\n","- [More Reading](#more-reading)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GLaIDetsrLFc"},"source":["# PCA = Principle Component Analysis\n","\n","**Goals**: \n","\n","- *Transform* original variable/features into new, \"high-performance\" features\n","- *Reduce* the dimensionality of the data\n","- *Eliminate* multicollinearity\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iEKvRtZfrLFl","slideshow":{"slide_type":"slide"}},"source":["<a id=\"motivation\"></a>\n","## Motivation\n","\n","### Dimensionality Reduction\n","\n","Dimensionality reduction reduces the number of random variables that you are considering for analysis until you are left with the most important variables.\n","\n","> Dimensionality reduction is not an end goal in itself, but a tool to form a dataset with more parsimonious features for further visualization and/or modelling.\n","\n","### Reducing Colinearity in Input\n","\n","To get a quick summary of our data, we can calculate a covariance matrix, an unstandardized correlation matrix.\n","\n","The diagonal elements in a covariance matrix show us the variance of each of our features.\n","\n","The off-diagnal elements show the covariance, the amount of colinearity and redundancy between our variables."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IVTWZYRArLFl"},"source":["## Motivating Example\n","\n","Say that I want to predict **age** from *stress*, *income* and *health*.\n","\n","1. Three-dimensional data\n","2. Multicollinearity probably exists\n","\n","*PCA* will give me one or two **super-predictor** variables called *components* (hopefully).\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t628Yiq_rLFn","slideshow":{"slide_type":"slide"}},"source":["<a id=\"whatispca\"></a>\n","## What is PCA?\n","\n","---\n","\n","PCA is the quintessential \"dimensionality reduction\" algorithm. \n","\n","_Dimensionality reduction_ is the process of combining or collapsing the existing features (columns in X) into fewer features. \n","\n","These hopefully:\n","\n","- Retain the signal in the original data, and\n","- Reduce noise."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7dLWI45orLFn","slideshow":{"slide_type":"fragment"}},"source":["---\n","\n","**Essentially...**\n","\n","- PCA finds *linear combinations* of current predictor variables that...\n","- create new \"principal components\". The principal components explain...\n","- the maximum possible amount of variance in your predictors.\n","\n","$$ PC1 = w_{1,1}(\\text{stress}) + w_{2,1}(\\text{income}) + w_{3,1}(\\text{health})$$\n","\n","$$ PC2 = w_{1,2}(\\text{stress}) + w_{2,2}(\\text{income}) + w_{3,2}(\\text{health})$$\n","\n","$$ PC3 = w_{1,3}(\\text{stress}) + w_{2,3}(\\text{income}) + w_{3,3}(\\text{health})$$\n","\n","This is cool because...\n","\n","- $PC1$ is more important than $PC2$ is better than $PC3$  \n","- All of these  $PCs$ are *uncorrelated*\n","\n","---\n","\n","**Visually...**\n","\n","> Think of PCA as a coordinate transformation.  The old axes are the original variables (columns). The new axes are the principal components from PCA.\n","\n","**The new axes (principal components) become the  most concise, informative descriptors of our data as a whole.**\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b-6OKxairLFo","slideshow":{"slide_type":"slide"}},"source":["<img alt=\"orthogonal eigenvectors\" src=\"https://drive.google.com/uc?export=view&id=117wER1h6aWurSSjghUUNjPBqpm9xydl4\">\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3GLdaSzarLFp","slideshow":{"slide_type":"slide"}},"source":["<img alt=\"[transformed XY\" src=\"https://drive.google.com/uc?export=view&id=1UfoiTOatAFGybhf3Kg8UG21s8ijw9ufY\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZuFTHbXLrLFp","slideshow":{"slide_type":"slide"}},"source":["<a id=\"pcs\"></a>\n","### Principal Components\n","\n","---\n","\n","- We are looking for new *directions* in feature space\n","- Each consecutive direction tries to maximize *remaining variance*\n","- Each direction is *orthogonal* to all the others\n","\n","**These new *directions* are the \"principal components\", i.e. the new coordinate system for your data.**\n","\n","> Applying PCA to your data *transforms* your original data columns (variables) onto the new principal component axes.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aCEhFFllrLFq"},"source":["(Did you catch that?  The *variables* define the *cordinate system*.)\n","\n","(CP1) **Let's review...**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"z4AnSP-DrLFr"},"source":["**The PCA transformation creates new variables that...**\n","1. Optimize \"explained variance\", and\n","2. Are uncorrelated.\n","\n","Creating these variables is a well-defined mathematical process. In essence, **each component is created as a weighted sum of your original columns, such that all components are orthogonal (perpendicular) to each other**.\n","\n","#### Example Continued\n","\n","The inputs `stress`, `income` and `health`...\n","\n","...can be *replaced* with 3 *new* variables...\n","\n","- `PC1` $\\rightarrow$ most variance\n","- `PC2`\n","- `PC3` $\\rightarrow$ least variance (noise?)\n","\n","---\n","\n","Mathematically:\n","\n","$$ PC1 = w_{1,1}(\\text{stress}) + w_{2,1}(\\text{income}) + w_{3,1}(\\text{health})$$\n","\n","$$ PC2 = w_{1,2}(\\text{stress}) + w_{2,2}(\\text{income}) + w_{3,2}(\\text{health})$$\n","\n","$$ PC3 = w_{1,3}(\\text{stress}) + w_{2,3}(\\text{income}) + w_{3,3}(\\text{health})$$\n","\n","The weights are called *loadings*... they are coefficients indicating how heavily each of the input data are weighted\n","\n","e.g.\n","\n","$$ PC1 = 0.01(\\text{stress}) - 0.54(\\text{income}) + 0.71(\\text{health})$$\n","\n","> We will see how to get these values from `sklearn`\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KlkHjhEKrLFr"},"source":["### Capturing variance\n","\n","The total variance of your data gets redistributed among the principal components:\n","\n","$$\\text{var}(PC1) > \\text{var}(PC2) > \\text{var}(PC3)$$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kZg1jyY5rLFs"},"source":["> #### Interpreting PCA: Signal v. Noise\n","\n","> PCA attempts to *maximize signal* (high variance) while *isolating noise* (low variance)\n","\n","> - Most variance captured in first several principal components\n","> - Noise isolated to last several principal compoments\n","> - This done simultaneously across *all input variables*\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wNGFVj71rLFs"},"source":["### Isolating variance\n","\n","There is no covariance between principal components\n","\n","$$\\text{covar}(PC1, PC2) = 0$$\n","\n","$$\\text{covar}(PC1, PC3) = 0$$\n","\n","$$\\text{covar}(PC2, PC3) = 0$$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KeJ7kppBrLFt"},"source":["**Two assumptions that PCA makes:**\n","1. **Linearity:** The data does not hold nonlinear relationships.\n","2. **Large variances define importance:** The dimensions are constructed to maximize remaining variance."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m4ZPJvSkrLFt","slideshow":{"slide_type":"slide"}},"source":["**PRINCIPAL COMPONENT TRANSFORMATION OF DATA: PC1 VS PC2**\n","\n","[setosa.io has an extremely nice interactive visualization for PCA](http://setosa.io/ev/principal-component-analysis/)\n","\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WAJmXZ3NrLFv","slideshow":{"slide_type":"-"}},"source":["<img alt=\"pca_coordinate_transformation\" src=\"https://drive.google.com/uc?export=view&id=1EoJBgCaeghbJGbx7SVwtx0IIUcji07lv\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"snFkXeKSrLFw"},"source":["### How we derive PCA\n","PCA relies on the...\n","\n","**Eigenvalue decomposition of the covariance matrix**\n","This diagonalizes the covariance matrix, therby eliminating covariance.\n","\n","**The principal component transformation**\n","This transforms each input variable $X$ onto a new orthogonal basis in which the new variables $Z$ are maximally variant.\n","\n","$$ \\mathbf{Z = WX} $$\n","\n","> * Sigma = Covariance Matrix (X)// X is the original feature data\n","> * [U,S,V] = svd(Sigma)//Perform Singular Value Decomposition\n","> * Ureduce = U(:,1:k) // k is the number of PCA components\n","> * W = Ureduce<sup>T</sup>\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"w99a2X7CrLF3","slideshow":{"slide_type":"slide"}},"source":["### Why would we want to do PCA?\n","\n","---\n","\n","- We can **reduce the number of dimensions** (remove less important components), while losing mostly noise rather than signal.\n","- Since we are assuming our variables are interrelated (at least in the sense that they together explain a dependent variable), the information of interest should exist along directions with largest variance.\n","- The directions of largest variance should have the highest signal-to-noise ratio.\n","- Correlated predictor variables (also referred to as \"redundancy\" of information) are combined into independent variables. Our predictors from PCA are guaranteed to be independent.\n","\n","---\n","\n","[Good paper on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n","\n","[Nice site on how PCA is done step by step with coding](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KYfN8Gf-rLF4","slideshow":{"slide_type":"slide"}},"source":["<a id=\"manual-codealong\"></a>\n","## PCA Codealong\n","\n","---\n","\n","**DATA**\n","\n","We are going to be using a simple 75-row, 4-column dataset with demographic information. It contains:\n","\n","    age (limited to 20-65)\n","    income\n","    health (a rating on a scale of 1-100, where 100 is the best health)\n","    stress (a rating on a scale of 1-100, where 100 is the most stressed)\n","    \n","All of the variables are continuous.\n","\n","---"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1WbzJM4cYp1I","colab":{}},"source":["#1 Code to read csv file into colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ubjxW9IrYrMG","colab":{}},"source":["#2. Get the file\n","#make sure you upload all your data files to your Google drive and change share->Advanced->change->anyone with the link can view\n","downloaded = drive.CreateFile({'id':'1FRTKbjLnzxDp0mJ3WHaFO-hYmwfXCW-V'}) # replace the id with id of file you want to access\n","downloaded.GetContentFile('simple_demographics.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p7c330SWrLF5","slideshow":{"slide_type":"fragment"},"outputId":"d9026ff1-c050-4991-80e4-5e8ffbe101e3","executionInfo":{"status":"ok","timestamp":1556849865506,"user_tz":420,"elapsed":4456,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","sns.set_style('white')\n","\n","demo = pd.read_csv('simple_demographics.csv')\n","demo.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(75, 4)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"hjj2fAb0XLDF","colab_type":"code","outputId":"282cbefa-9379-4a29-928b-0988394e3ba0","executionInfo":{"status":"ok","timestamp":1556849865508,"user_tz":420,"elapsed":4424,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["demo.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>health</th>\n","      <th>income</th>\n","      <th>stress</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>21</td>\n","      <td>74.0</td>\n","      <td>42746.0</td>\n","      <td>53.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>33</td>\n","      <td>64.0</td>\n","      <td>72792.0</td>\n","      <td>49.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30</td>\n","      <td>78.0</td>\n","      <td>74178.0</td>\n","      <td>64.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>32</td>\n","      <td>71.0</td>\n","      <td>102548.0</td>\n","      <td>63.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57</td>\n","      <td>46.0</td>\n","      <td>120418.0</td>\n","      <td>35.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age  health    income  stress\n","0   21    74.0   42746.0    53.0\n","1   33    64.0   72792.0    49.0\n","2   30    78.0   74178.0    64.0\n","3   32    71.0  102548.0    63.0\n","4   57    46.0  120418.0    35.0"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"L2rXBWfBrLF7","slideshow":{"slide_type":"slide"}},"source":["<a id=\"basic-eda\"></a>\n","### 1. Basic EDA\n","\n","Make a seaborn pairplot for the dataset to see how each feaure is correlated to the other.\n","\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r8_semgurLF_","slideshow":{"slide_type":"slide"}},"source":["<a id=\"subset\"></a>\n","### 2. Subset and normalize\n","\n","Subset the data to only include:\n","\n","    income\n","    health\n","    stress\n","\n","We will be comparing the principal components to age specifically, so we are leaving age out.\n","\n","---"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5JPZNEaMrLF_","slideshow":{"slide_type":"fragment"},"colab":{}},"source":["demo_noage = demo[['health','income','stress']]\n","#Let's normalize the data\n","demo_noage = (demo_noage - demo_noage.mean()) / demo_noage.std()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y2kLOsN5rLGC","slideshow":{"slide_type":"slide"}},"source":["<a id=\"corr\"></a>\n","### 3. Calculate correlation matrix\n","\n","We will be using the correlation matrix to calculate the eigenvectors and eigenvalues.\n","\n","---"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"09wd2jVNrLGD","outputId":"1ace1ca5-3ebf-421c-e661-7c6187749782","slideshow":{"slide_type":"fragment"},"executionInfo":{"status":"ok","timestamp":1556849865512,"user_tz":420,"elapsed":4380,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["# demo_noage_corr = np.corrcoef(demo_noage.values.T)\n","demo_noage.corr()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>health</th>\n","      <th>income</th>\n","      <th>stress</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>health</th>\n","      <td>1.000000</td>\n","      <td>0.192037</td>\n","      <td>0.527663</td>\n","    </tr>\n","    <tr>\n","      <th>income</th>\n","      <td>0.192037</td>\n","      <td>1.000000</td>\n","      <td>-0.347925</td>\n","    </tr>\n","    <tr>\n","      <th>stress</th>\n","      <td>0.527663</td>\n","      <td>-0.347925</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          health    income    stress\n","health  1.000000  0.192037  0.527663\n","income  0.192037  1.000000 -0.347925\n","stress  0.527663 -0.347925  1.000000"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NIaBg2d7rLGF","slideshow":{"slide_type":"slide"}},"source":["<a id=\"eigen\"></a>\n","### 4. Calculate the principal components and associated explained variances\n","\n","---"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JcslVQZirLGG","outputId":"b9eec57e-e458-4151-cd98-66c880cc20f5","executionInfo":{"status":"ok","timestamp":1556849865514,"user_tz":420,"elapsed":4358,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["from sklearn.decomposition import PCA\n","\n","X = demo_noage\n","\n","pca = PCA()\n","pca = pca.fit(X)\n","\n","print(pca.explained_variance_)\n","print(pca.components_)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1.55645677 1.17357375 0.26996948]\n","[[-0.6187659   0.25173885 -0.74414804]\n"," [ 0.5126449   0.84716255 -0.13968116]\n"," [-0.59525118  0.46791364  0.65324793]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VyztFwMmrLGK"},"source":["Interpreting the \"components\": recall that\n","\n","$$ PC1 = w_{1,1}(\\text{health}) + w_{2,1}(\\text{income}) + w_{3,1}(\\text{stress})$$\n","\n","$$ PC2 = w_{1,2}(\\text{health}) + w_{2,2}(\\text{income}) + w_{3,2}(\\text{stress})$$\n","\n","$$ PC3 = w_{1,3}(\\text{health}) + w_{2,3}(\\text{income}) + w_{3,3}(\\text{stress})$$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xRTm7QQvrLGZ","slideshow":{"slide_type":"slide"}},"source":["<a id=\"transformed\"></a>\n","### 5. Construct the Transformed Data Set $Z$\n","\n","---"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X1K7uI4VrLGa","outputId":"1a53e078-0f95-4b92-b8a3-b6a92c0bd9cd","executionInfo":{"status":"ok","timestamp":1556849865515,"user_tz":420,"elapsed":4334,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":202}},"source":["Z = pca.transform(demo_noage)\n","\n","features_pca = ['PC'+str(i+1) for i in range(pca.n_components_)]\n","Z = pd.DataFrame(Z, columns=features_pca)\n","Z.head(5)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PC1</th>\n","      <th>PC2</th>\n","      <th>PC3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.882350</td>\n","      <td>-0.150070</td>\n","      <td>-1.166109</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.104542</td>\n","      <td>0.102205</td>\n","      <td>-0.597921</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-1.372329</td>\n","      <td>0.496715</td>\n","      <td>-0.563529</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.867671</td>\n","      <td>0.797774</td>\n","      <td>-0.017223</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.566225</td>\n","      <td>0.491018</td>\n","      <td>0.087187</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        PC1       PC2       PC3\n","0 -0.882350 -0.150070 -1.166109\n","1 -0.104542  0.102205 -0.597921\n","2 -1.372329  0.496715 -0.563529\n","3 -0.867671  0.797774 -0.017223\n","4  1.566225  0.491018  0.087187"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"vwt7bRGD8yd0","colab_type":"code","outputId":"6413d2af-5bde-4d1a-e421-d3f52aaf2ec2","executionInfo":{"status":"ok","timestamp":1556849865517,"user_tz":420,"elapsed":4315,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["Z.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(75, 3)"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"z6OHkuNsrLGd","outputId":"aaf509ba-dc7e-463b-badc-737a7889c362","executionInfo":{"status":"ok","timestamp":1556849865518,"user_tz":420,"elapsed":4280,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":140}},"source":["Z.corr()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PC1</th>\n","      <th>PC2</th>\n","      <th>PC3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>PC1</th>\n","      <td>1.000000e+00</td>\n","      <td>7.531902e-16</td>\n","      <td>6.017637e-17</td>\n","    </tr>\n","    <tr>\n","      <th>PC2</th>\n","      <td>7.531902e-16</td>\n","      <td>1.000000e+00</td>\n","      <td>1.239421e-16</td>\n","    </tr>\n","    <tr>\n","      <th>PC3</th>\n","      <td>6.017637e-17</td>\n","      <td>1.239421e-16</td>\n","      <td>1.000000e+00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              PC1           PC2           PC3\n","PC1  1.000000e+00  7.531902e-16  6.017637e-17\n","PC2  7.531902e-16  1.000000e+00  1.239421e-16\n","PC3  6.017637e-17  1.239421e-16  1.000000e+00"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zn57smB2rLG_"},"source":["### 6. PCA applied to prediction problems"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Smh7CLsUrLHA"},"source":["Now build out a base-line linear regression model predicting `age` from `health`, `income` and `stress`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NY9PLqZ9rLHB","outputId":"ab93378c-200a-4245-898c-0539ae890682","executionInfo":{"status":"ok","timestamp":1556849865850,"user_tz":420,"elapsed":4588,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import cross_val_score\n","\n","features = ['health', 'income', 'stress']\n","\n","X = demo[features]\n","y = demo['age']\n","\n","ss = StandardScaler()\n","Xs = ss.fit_transform(X)\n","\n","lr = LinearRegression()\n","\n","print('Cross validataion score with Linear Regression: %.2f%%' %(cross_val_score(lr, Xs, y, cv=5).mean()*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cross validataion score with Linear Regression: 85.56%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zBeex0tErLHH"},"source":["### Repeat the linear regression, but reduce the dimensionality to 2 (instead of 3) using PCA.  "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cl0tR0WArLHH","outputId":"2278ac00-8c10-4e85-a3db-e8333dcfa4e0","executionInfo":{"status":"ok","timestamp":1556849865852,"user_tz":420,"elapsed":4561,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["from sklearn.decomposition import PCA\n","\n","X = demo[features]\n","y = demo['age']\n","\n","ss = StandardScaler()\n","Xs = ss.fit_transform(X)\n","\n","pca = PCA(n_components=2)\n","Xt = pca.fit_transform(Xs)\n","\n","lr = LinearRegression()\n","\n","print('Cross validataion score with Linear Regression and 2 PCA components: %.2f%%' %(cross_val_score(lr, Xt, y, cv=5).mean()*100))\n","\n","pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=features)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cross validataion score with Linear Regression and 2 PCA components: 72.24%\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PC1</th>\n","      <th>PC2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>health</th>\n","      <td>-0.618766</td>\n","      <td>0.512645</td>\n","    </tr>\n","    <tr>\n","      <th>income</th>\n","      <td>0.251739</td>\n","      <td>0.847163</td>\n","    </tr>\n","    <tr>\n","      <th>stress</th>\n","      <td>-0.744148</td>\n","      <td>-0.139681</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             PC1       PC2\n","health -0.618766  0.512645\n","income  0.251739  0.847163\n","stress -0.744148 -0.139681"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oaVqkTH1rLHJ","colab":{}},"source":["def pca_performance(Xs,n):    \n","#write a function to try out different PCA components\n","#print out the prediction\n","  pca = PCA(n_components=n)\n","  Xt = pca.fit_transform(Xs)\n","\n","  lr = LinearRegression()\n","\n","  print('Cross validataion score with Linear Regression and %d PCA components: %.2f%%' %(n,cross_val_score(lr, Xt, y, cv=5).mean()*100))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PnV2YnKyrLHK","outputId":"8fba22ce-a902-4020-c3a7-1c2823159750","executionInfo":{"status":"ok","timestamp":1556849865862,"user_tz":420,"elapsed":4512,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["pca_performance(Xs,1)\n","pca_performance(Xs,2)\n","pca_performance(Xs,3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cross validataion score with Linear Regression and 1 PCA components: 69.92%\n","Cross validataion score with Linear Regression and 2 PCA components: 72.24%\n","Cross validataion score with Linear Regression and 3 PCA components: 85.56%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3ld9Ba4ZsVvi"},"source":["## Apply PCA to Digits Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jtof4cVosVvk","colab":{}},"source":["# starter code \n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","digits = load_digits()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pjmGr7x7r2-x","colab":{}},"source":["X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SmS_pVDfsVv9","colab":{}},"source":["# normalizing the dataset \n","ss = StandardScaler()\n","X_train = ss.fit_transform(X_train)\n","X_test = ss.transform(X_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XiyV3RtCpIJb","outputId":"0583f665-993d-420d-e3b4-5af6e5da2b85","executionInfo":{"status":"ok","timestamp":1556849866415,"user_tz":420,"elapsed":4998,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["%%time\n","#Let's see how good is Logistic Regression\n","logit = LogisticRegression()\n","logit.fit(X_train, y_train)\n","print (\"Logistic Regression Accuracy: %.2f%%\" %(logit.score(X_test, y_test)*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n","  \"this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Logistic Regression Accuracy: 96.11%\n","CPU times: user 419 ms, sys: 2.55 ms, total: 422 ms\n","Wall time: 429 ms\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_1BrpyD4im52","outputId":"debd068c-bd6b-423d-821a-3c6158a68249","executionInfo":{"status":"ok","timestamp":1556849866417,"user_tz":420,"elapsed":4981,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["%%time\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=16)\n","X_train_t = pca.fit_transform(X_train)\n","X_test_t = pca.transform(X_test)\n","logit = LogisticRegression()\n","logit.fit(X_train_t, y_train)\n","print (\"Logistic Regression Accuracy with %d PCA components: %.2f%%\" %(16,logit.score(X_test_t, y_test)*100))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Logistic Regression Accuracy with 16 PCA components: 93.33%\n","CPU times: user 152 ms, sys: 94.3 ms, total: 246 ms\n","Wall time: 129 ms\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n","  \"this warning.\", FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"OfF1jOHZbzWo","colab_type":"code","colab":{}},"source":["#write a function\n","#digit_pca(X_train,X_test,y_train,y_test, n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jFqW5mJFlEVY","outputId":"6ac50f6f-0246-4f69-9c53-5485459a2b76","executionInfo":{"status":"error","timestamp":1556849866687,"user_tz":420,"elapsed":5244,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":400}},"source":["%%time\n","digit_pca(X_train,X_test,y_train,y_test, 64)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-3f29823ce387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'digit_pca(X_train,X_test,y_train,y_test, 64)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m</usr/local/lib/python3.6/dist-packages/decorator.py:decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'digit_pca' is not defined"]}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OlQrzQnlmeHf","colab":{}},"source":["%%time\n","digit_pca(X_train,X_test,y_train,y_test, 32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uOEyHejsmn3k","colab":{}},"source":["%%time\n","digit_pca(X_train,X_test,y_train,y_test, 16)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OqIDbtozmrLU","colab":{}},"source":["%%time\n","digit_pca(X_train,X_test,y_train,y_test, 8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sDfGhL-DsPyK"},"source":["## Digit recognition with Manifold Learning"]},{"cell_type":"markdown","metadata":{"id":"3GnCZuGuGzKw","colab_type":"text"},"source":["<img src=\"http://benalexkeen.com/wp-content/uploads/2017/05/isomap.png\" style=\"float: left; margin: 20px; height: 75px\">"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JH_pVQqBpYlx","colab":{}},"source":["from sklearn.manifold import Isomap\n","def digit_manifold(X_train,X_test,y_train,y_test, n):\n","  model = Isomap(n_components=n)\n","  X_train_t = model.fit_transform(X_train)\n","  X_test_t = model.transform(X_test)\n","  logit = LogisticRegression()\n","  logit.fit(X_train_t, y_train)\n","  print (\"Logistic Regression Accuracy with %d Isomap components: %.2f%%\" %(n,logit.score(X_test_t, y_test)*100))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mTkQeunDsvmQ","outputId":"b468bbd6-5e76-423e-d03b-80756eab44bb","executionInfo":{"status":"ok","timestamp":1556849890225,"user_tz":420,"elapsed":3231,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["%%time\n","digit_manifold(X_train,X_test,y_train,y_test, 64)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n","  \"this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Logistic Regression Accuracy with 64 Isomap components: 96.11%\n","CPU times: user 2.86 s, sys: 286 ms, total: 3.14 s\n","Wall time: 2.63 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aFaLDrW3evNN","colab_type":"code","outputId":"259f1dc1-a472-4359-bd77-7dba9e6c6c21","executionInfo":{"status":"ok","timestamp":1556849892306,"user_tz":420,"elapsed":5297,"user":{"displayName":"Chris Tseng","photoUrl":"","userId":"17545854703594359033"}},"colab":{"base_uri":"https://localhost:8080/","height":158}},"source":["%%time\n","digit_manifold(X_train,X_test,y_train,y_test, 32)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n","  \"this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Logistic Regression Accuracy with 32 Isomap components: 95.83%\n","CPU times: user 2.59 s, sys: 237 ms, total: 2.83 s\n","Wall time: 2.22 s\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-7KunpfArLHT","slideshow":{"slide_type":"slide"}},"source":["<a id=\"more-reading\"></a>\n","### More useful links, reading, and references for images\n","\n","---\n","\n","[PCA 4 dummies](https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)\n","\n","[Stackoverflow making sense of PCA](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n","\n","[PCA and spectral theorem](http://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit)\n","\n","[PCA in 3 steps: eigendecomposition and SVD](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#pca-vs-lda)\n","\n","[Tutorial on PCA](http://arxiv.org/pdf/1404.1100.pdf)\n","\n","[PCA math and examples](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch18.pdf)"]}]}