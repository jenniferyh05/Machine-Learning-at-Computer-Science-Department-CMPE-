{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of ensemble-methods.ipynb","version":"0.3.2","provenance":[{"file_id":"1HwsPmUSN841wqE7oJpQ-ekZcc_TnFM8Y","timestamp":1550805507795}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"FmQhJP3L1vUh","colab_type":"text"},"cell_type":"markdown","source":["\n","# Bootstrapping, Bagging, Random Forests, and ExtraTrees"]},{"metadata":{"id":"5Mi_ae4u1vUi","colab_type":"text"},"cell_type":"markdown","source":["## Introduction to Ensemble Methods\n","We can list out the different types of models we've built thus far:\n","- Linear Regression\n","- Generalized Linear Models\n","    - Logistic Regression\n","    - Multinomial Logistic Regression\n","    - Poisson Regression\n","- $k$-Nearest Neighbors\n","- Naive Bayes Classification\n","\n","If we want to use any of these models, we follow the same type of process.\n","1. Based on our problem, we identify which model to use. (Is our problem classification or regression? Do we want an interpretable model?)\n","2. Fit the model using the training data.\n","3. Use the fit model to generate predictions.\n","4. Evaluate our model's performance and, if necessary, return to step 2 and make changes.\n","\n","In this case, we've always had exactly one model. Today, however, we're going to talk about **ensemble methods**. Mentally, you should think about this as if we build multiple models and then aggregate their results in some way.\n","\n","### Why would we build an \"ensemble model?\"\n","Consider $H$ the space of all possible hypotheses. Our goal is to estimate $f$, the true function. We can come up with different hypotheses $h_1$, $h_2$, and so on to get as close to $f$ as possible. \n","- Think about $f$ as the true process that dictates Iowa liquor sales.\n","- Think about $h_1$ as the model you built to predict $f$.\n","\n","\n","- The **statistical** benefit to ensemble methods: By building one model, our predictions are almost certainly going to be wrong. Predictions from one model might overestimate liquor sales; predictions from another model might underestimate liquor sales. By \"averaging\" predictions from multiple models, we'll see that we can often cancel our errors out and get closer to the true function $f$.\n","- The **computational** benefit to ensemble methods: It might be impossible to develop one model that globally optimizes our objective function. (Remember that CART reach locally-optimal solutions and that all generalized linear models iterate toward a solution that isn't guaranteed to be the globally-optimal solution.) In these cases, it may be impossible for one CART or an individual GLM to arrive at the true function $f$. However, starting our \"local searches\" at different points and aggregating our predictions might .\n","- The **representational** benefit to ensemble methods: Even if we had all the data and all the computer power in the world, ot might be impossible for one model to exactly equal $f$. For example, a linear regression model can never model a relationship where a one-unit change in $X$ effects some *different* change in $Y$ based on the value of $X$. All models have some shortcomings. (See [the no free lunch theorems](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization).) While individual models have shortcomings, by creating multiple models and aggregating their predictions, we can actually create predictions that represent something that one model cannot ever represent."]},{"metadata":{"id":"uGQPf46G1vUj","colab_type":"text"},"cell_type":"markdown","source":["## Bootstrapping\n","\n","Let's get started actually making ensemble predictions. However, in order to do that, we'll need to introduce the idea of bootstrapping, or random sampling **with replacement.**\n","\n","---\n","\n"]},{"metadata":{"id":"lfvj_Wx71vUk","colab_type":"code","outputId":"cfa8e521-0134-4c10-c81a-9371fd25aff3","executionInfo":{"status":"ok","timestamp":1551072931632,"user_tz":480,"elapsed":473,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":113}},"cell_type":"code","source":["# scikit-learn bootstrap\n","from sklearn.utils import resample\n","# data sample\n","data = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n","# prepare bootstrap sample\n","boot = resample(data, replace=True, n_samples=4)\n","print('Bootstrap Sample: %s' % boot)\n","# out of bag observations\n","oob = [x for x in data if x not in boot]\n","print('OOB Sample: %s' % oob)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Bootstrap Sample: [0.5, 0.4, 0.1, 0.2]\n","OOB Sample: [0.3, 0.6]\n"],"name":"stdout"}]},{"metadata":{"id":"dSdKkZUe1vUo","colab_type":"text"},"cell_type":"markdown","source":["## Bagged Decision Trees\n","\n","As we have seen, decision trees are very powerful machine learning models. However, decision trees have some limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns (a.k.a. they overfit their training sets). \n","\n","\n","Bagging (bootstrap aggregating) helps to mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n","\n","The process for creating bagged decision trees is as follows:\n","1. From the original data of size $n$, bootstrap $k$ samples each of size $n$ (with replacement!).\n","2. Build a decision tree on each bootstrapped sample.\n","3. Make predictions by passing a test observation through all $k$ trees and developing one aggregate prediction for that observation.\n","\n","\n","<a href='http://www.cse.buffalo.edu/~jing/sdm10ensemble.htm'>\n","    <img src='./assets/images/Ensemble.png'/>\n","</a>\n","\n","### What do you mean by \"aggregate prediction?\"\n","As with all of our modeling techniques, we want to make sure that we can come up with one final prediction for our observation. (Building 1,000 trees and coming up with 1,000 predictions for one observation probably wouldn't be very helpful.)\n","\n","Suppose we want to predict whether or not a Reddit post is going to go viral, where `1` indicates viral and `0` indicates non-viral. We build 100 decision trees. Given a new Reddit post labeled `X_test`, we pass these features into all 100 decision trees.\n","- 70 of the trees predict that the post in `X_test` will go viral.\n","- 30 of the trees predict that the post in `X_test` will not go viral.\n","\n","<details><summary> What might you expect `.predict(X_test)` and `.predict_proba(X_test)` to do?\n","</summary>\n","```\n","- .predict(X_test) should output a 1, predicting that the post will go viral.\n","- .predict_proba(X_test) should output 0.7, indicating the probability of the post going viral is 70%.\n","```\n","</details>\n","\n","- **Discrete:** In ensemble methods, we will most commonly predict a discrete $Y$ by \"plurality vote,\" where the most common class is the predicted value for a given observation.\n","- **Continuous:** In ensemble methods, we will most commonly predict a continuous $Y$ by averaging the predicted values into one final prediction.\n","\n","---\n","\n","## Random Forests\n","\n","With bagged decision trees, we generate many different trees on pretty similar data. These trees are **strongly correlated** with one another. Because these trees are correlated with one another, they will have high variance. Looking at the variance of the average of two random variables $T_1$ and $T_2$:\n","\n","$$\n","\\begin{eqnarray*}\n","Var\\left(\\frac{T_1+T_2}{2}\\right) &=& \\frac{1}{4}\\left[Var(T_1) + Var(T_2) + 2Cov(T_1,T_2)\\right]\n","\\end{eqnarray*}\n","$$\n","\n","If $T_1$ and $T_2$ are highly correlated, then the variance will about as high as we'd see with individual decision trees. By \"decorrelating\" our trees from one another, we can drastically reduce the variance of our model.\n","\n","That's the difference between bagged decision trees and random forests! We're going to do the same thing as before, but we're going to decorrelate our trees. This will reduce our variance (at the expense of a small increase in bias) and thus should greatly improve the overall performance of the final model.\n","\n","### So how do we \"decorrelate\" our trees?\n","Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each split in the learning process, a **random subset of the features**. This process is sometimes called the *random subspace method*.\n","\n","The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be used in many/all of the bagged decision trees, causing them to become correlated. By selecting a random subset of the features at each split, we counter this correlation between base trees, strengthening the overall model.\n","\n","For a problem with $p$ features, it is typical to use:\n","\n","- $\\sqrt{p}$ (rounded down) features in each split for a classification problem.\n","- $p/3$ (rounded down) with a minimum node size of 5 as the default for a regression problem.\n","\n","While this is a guideline, Hastie and Tibshirani (authors of Introduction to Statistical Learning and Elements of Statistical Learning) have suggested this as a good rule in the absence of some rationale to do something different.\n","\n","Random forests, a step beyond bagged decision trees, are **very widely used** classifiers and regressors. They are relatively simple to use because they require very few parameters to set and they perform pretty well.\n","- It is quite common for interviewers to ask how a random forest is constructed or how it is superior to a single decision tree.\n","\n","--- \n","\n","## Extremely Randomized Trees (ExtraTrees)\n","Adding one more step of randomization (and thus decorrelation) yields extremely randomized trees, or _ExtraTrees_. These are trained using bagging (sampling of observations) and the random subspace method (sampling of features), but an additional layer of randomness is introduced. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity) for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range.\n","\n","This further reduces the variance, but causes an increase in bias. If you're considering using ExtraTrees, you might consider this to be a hyperparameter you can tune. Build an ExtraTrees model, a Random Forest model, and a Bagged Decision Treesmodel, then compare their performance!\n","\n","That's exactly what we'll do below."]},{"metadata":{"id":"-eJx5o3z1vUo","colab_type":"text"},"cell_type":"markdown","source":["## Guided Practice: Random Forest and ExtraTrees in Scikit Learn\n","\n","Scikit Learn implements both random forest and extra trees methods as part of the `ensemble` module.\n","\n","Have a look at the [documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest).\n","\n","**Check:** What parameters did you notice? Any questions on those?\n","\n","Let's load the [car dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/).\n","\n","We would like to compare the performance of the following 4 algorithms:\n","\n","- Decision Trees\n","- Bagging + Decision Trees\n","- Random Forest\n","- Extra Trees\n","\n","Note that in order for our results to be consistent, we have to expose the models to exactly the same CrossValidation scheme. Let's start by initializing that. Then, we'll initialize the models.\n","\n","*You can also create a function to speed up your work...*"]},{"metadata":{"colab_type":"code","id":"1WbzJM4cYp1I","outputId":"be69b5a7-9a44-4509-df3d-2a8767f3210a","executionInfo":{"status":"ok","timestamp":1551072993040,"user_tz":480,"elapsed":61867,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":266}},"cell_type":"code","source":["#1 Code to read csv file into colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# 1. Authenticate and create the PyDrive client.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 19.0MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.4MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.9MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.2MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.9MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.5MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.9MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.5MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 5.2MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 5.2MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 7.1MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 7.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 13.0MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 13.2MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 13.5MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 13.0MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 13.0MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 13.1MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 42.6MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 15.3MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 15.3MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 15.5MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 15.3MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 15.3MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 14.7MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 15.4MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 15.4MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 15.3MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 15.6MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 47.9MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 48.5MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 49.2MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 44.0MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 44.0MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 50.5MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 50.3MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 50.7MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 20.0MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 19.6MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 19.7MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 19.3MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 19.5MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 20.0MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 19.8MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 19.8MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 19.8MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 19.9MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 50.5MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 49.1MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 49.6MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 52.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 51.9MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 57.6MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 58.9MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 58.1MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 57.3MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 55.1MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 54.8MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 59.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 58.3MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 59.1MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 57.9MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 55.8MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 43.9MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 28.0MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 27.8MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 28.1MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 28.0MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 28.0MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 28.1MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 28.0MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 28.3MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 28.4MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 32.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 55.8MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 58.0MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 59.7MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 60.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 60.7MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 59.6MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 58.6MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 58.4MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 51.8MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 53.2MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 53.4MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 53.7MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 53.2MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 53.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 53.1MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 53.7MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 54.0MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 53.4MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 61.5MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 62.2MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 61.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 10.9MB/s \n","\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n","\u001b[?25h"],"name":"stdout"}]},{"metadata":{"colab_type":"code","id":"ubjxW9IrYrMG","colab":{}},"cell_type":"code","source":["#2. Get the file\n","#make sure you upload all your data files to your Google drive and change share->Advanced->change->anyone with the link can view\n","downloaded = drive.CreateFile({'id':'1LFfAsD23aNrgYdwGCAumbyIYzu2F4Shz'}) # replace the id with id of file you want to access\n","downloaded.GetContentFile('car.csv')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dD3iO7R01vUp","colab_type":"code","outputId":"e5b919e6-6945-4e26-bc82-950753f4dc55","executionInfo":{"status":"ok","timestamp":1551072994049,"user_tz":480,"elapsed":62860,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":261}},"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('car.csv')\n","df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>buying</th>\n","      <th>maint</th>\n","      <th>doors</th>\n","      <th>persons</th>\n","      <th>lug_boot</th>\n","      <th>safety</th>\n","      <th>acceptability</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>small</td>\n","      <td>low</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>small</td>\n","      <td>med</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>small</td>\n","      <td>high</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>med</td>\n","      <td>low</td>\n","      <td>unacc</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>vhigh</td>\n","      <td>vhigh</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>med</td>\n","      <td>med</td>\n","      <td>unacc</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  buying  maint doors persons lug_boot safety acceptability\n","0  vhigh  vhigh     2       2    small    low         unacc\n","1  vhigh  vhigh     2       2    small    med         unacc\n","2  vhigh  vhigh     2       2    small   high         unacc\n","3  vhigh  vhigh     2       2      med    low         unacc\n","4  vhigh  vhigh     2       2      med    med         unacc"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"H6DS1fsJYw7f","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"8srbYBc41vUs","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","y = LabelEncoder().fit_transform(df['acceptability'])\n","X = pd.get_dummies(df.drop('acceptability', axis=1))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aAy4L7KOTe36","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"XAb_lP541vUt","colab_type":"code","outputId":"f4ed8fa8-722f-4295-93da-3199f79cc6cd","executionInfo":{"status":"ok","timestamp":1551072994226,"user_tz":480,"elapsed":63015,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":160}},"cell_type":"code","source":["pd.Series(y).value_counts()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2    1210\n","0     384\n","1      69\n","3      65\n","dtype: int64"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"AwxWUD5z1vUw","colab_type":"code","colab":{}},"cell_type":"code","source":["#Find out how many different values and how often they appear in y\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1oCYreTi1vUy","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n","\n","cv = StratifiedKFold(n_splits=3, shuffle=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oNQK2jZ-1vU0","colab_type":"code","outputId":"6935c28d-0987-47cb-a2ad-cba71b42fe11","executionInfo":{"status":"ok","timestamp":1551072996429,"user_tz":480,"elapsed":65196,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":94}},"cell_type":"code","source":["dt = DecisionTreeClassifier()\n","s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Decision Tree Score:\t0.949 ± 0.004\n"],"name":"stdout"}]},{"metadata":{"id":"-sqq7LmJ1vU2","colab_type":"code","outputId":"e3764298-e526-407c-c239-031435d6e6f3","executionInfo":{"status":"ok","timestamp":1551072996433,"user_tz":480,"elapsed":65184,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":94}},"cell_type":"code","source":["#Repeat for BaggingClassifier()\n","dt2 = BaggingClassifier()\n","s = cross_val_score(dt2, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Bagging Tree\", s.mean().round(3), s.std().round(3)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Bagging Tree Score:\t0.962 ± 0.005\n"],"name":"stdout"}]},{"metadata":{"id":"uv1lBRRQ1vU4","colab_type":"code","outputId":"cf4d2717-42d1-4a8f-9e0f-c0c4c3a71241","executionInfo":{"status":"ok","timestamp":1551072996603,"user_tz":480,"elapsed":65338,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":94}},"cell_type":"code","source":["#Repeat for RandomForestClassifier()\n","dt3 = RandomForestClassifier()\n","s = cross_val_score(dt3, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest\", s.mean().round(3), s.std().round(3)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Random Forest Score:\t0.942 ± 0.005\n"],"name":"stdout"}]},{"metadata":{"id":"-F26KMfq1vU6","colab_type":"code","outputId":"f9144294-d495-47b8-ac2d-d702d453a56e","executionInfo":{"status":"ok","timestamp":1551072996605,"user_tz":480,"elapsed":65312,"user":{"displayName":"Yihui Zhou","photoUrl":"https://lh4.googleusercontent.com/-ljlKBDShz2I/AAAAAAAAAAI/AAAAAAAAAAc/KPadYeKTZ7o/s64/photo.jpg","userId":"09102422437475547191"}},"colab":{"base_uri":"https://localhost:8080/","height":94}},"cell_type":"code","source":["#Repeat for ExtraTreesClassifier()\n","dt4 = ExtraTreesClassifier()\n","s = cross_val_score(dt4, X, y, cv=cv, n_jobs=-1)\n","print(\"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Extra Tree\", s.mean().round(3), s.std().round(3)))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Extra Tree Score:\t0.939 ± 0.002\n"],"name":"stdout"}]},{"metadata":{"id":"lEZkVz4w1vU9","colab_type":"text"},"cell_type":"markdown","source":["Let's explore the effect of balancing our classes on the score."]},{"metadata":{"id":"hjsGzoI-1vU-","colab_type":"text"},"cell_type":"markdown","source":["## Conclusion\n","\n","We can improve the performance of a single model by generating multiple models and aggregating their predictions. \n","- If our $Y$ is continuous, we average each prediction. \n","- If our $Y$ is discrete, we use a \"plurality vote\" to decide the predicted value.\n","\n","The three types of ensemble models we discussed today:\n","1. **Bagged Decision Trees** are where we take the original data of size $n$, bootstrap $k$ samples each of size $n$ (with replacement!), build a decision tree on each sample, then make predictions by passing a test observation through all $k$ trees and developing one aggregate prediction for that observation.\n","2. **Random Forests** are where we bag decision trees, but when it comes to building each individual decision tree, we only consider a random subset of features at each split.\n","3. **ExtraTrees** are where we build random forests, but when it comes to building each individual decision tree, we also select a random split of each feature at each node.\n","\n","Some of these methods will perform better in some cases, some better in other cases. For example, decision trees are more nimble and easier to communicate, but have a tendency to overfit. On the other hand, ensemble methods perform better in more complex scenarios, but may become very complicated and harder to explain. (This gets back to our discussion about prediction versus inference - only you and your stakeholders will recognize what balance to strike between these two!)"]},{"metadata":{"id":"7X-KHWOb1vU_","colab_type":"text"},"cell_type":"markdown","source":["### ADDITIONAL RESOURCES\n","\n","- [Random Forest on wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n","- [Quora question on Random Forest](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n","- [Scikit Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n","- [Scikit Learn Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n","- [Ensemble Methods Paper](http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf)"]}]}