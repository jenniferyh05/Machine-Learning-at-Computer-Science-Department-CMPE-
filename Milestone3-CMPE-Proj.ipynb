{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Milestone3-CMPE-Proj.ipynb","version":"0.3.2","provenance":[{"file_id":"1Uj-XUEA1qdWaXcqoXkPzpJPU4WWUAP-B","timestamp":1557294229061},{"file_id":"https://gist.github.com/polycoderguo/0b94ec0f10682789f0bfa4beff47c66c#file-25-models-milestone2-cmpe-proj-ipynb","timestamp":1556910642792}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-tvtM8g_O6JJ","colab_type":"text"},"source":["<center>\n","  <font size=\"4\">CIFAR100 Image Classification</font><br>\n","  <font size=\"3\">  -- Milestone 3</font><br>\n","  Xiyu Cao,\tMeng Wang, Yuxue Jiang, Mengguo Yan, Qianhui Lu, Yihui Zhou</center>\n","\n"]},{"cell_type":"code","metadata":{"id":"q1G2N91N7RBF","colab_type":"code","outputId":"a38de789-0353-482a-9112-8532fc5c1baa","executionInfo":{"status":"ok","timestamp":1556920759921,"user_tz":420,"elapsed":6141,"user":{"displayName":"Mengguo Yan","photoUrl":"","userId":"13426425900381067474"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","from __future__ import print_function\n","import tensorflow as tf\n","import keras\n","from keras.datasets import cifar100\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Flatten, Activation\n","\n","from keras.layers.convolutional import Conv2D\n","from keras.optimizers import Adam\n","from keras.layers.pooling import MaxPooling2D\n","from keras.callbacks import ModelCheckpoint,EarlyStopping\n","from keras.utils import to_categorical\n","from keras.models import load_model\n","from keras.optimizers import SGD\n","from keras.regularizers import l1, l2\n","from keras.utils import np_utils\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from math import *\n","import warnings\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import time\n","from sklearn.decomposition import PCA\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report, confusion_matrix"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"cnRWDP2yKFXt","colab_type":"text"},"source":["### Data preparation\n","1) Data first loaded from external site and label preparation"]},{"cell_type":"code","metadata":{"id":"EGoY6qIXId0m","colab_type":"code","outputId":"2a25d714-1cb3-431a-90d4-9accdb2e7bd0","executionInfo":{"status":"ok","timestamp":1556920768678,"user_tz":420,"elapsed":8063,"user":{"displayName":"Mengguo Yan","photoUrl":"","userId":"13426425900381067474"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["def PreprocessDataset():\n","    (x_train_org, y_train_org), (x_test_org, y_test_org) = cifar100.load_data(label_mode='fine')\n","    x_train_org = x_train_org.astype('float32')\n","    x_test_org = x_test_org.astype('float32')\n","    # Normalize value to [0, 1]\n","    x_train_org /= 255\n","    x_test_org /= 255\n","    # Transform lables to one-hot\n","    y_train_org = np_utils.to_categorical(y_train_org, 100)\n","    y_test_org = np_utils.to_categorical(y_test_org, 100)\n","    # Reshape: here x_train is re-shaped to [channel] × [width] × [height]\n","    # In other environment, the orders could be different; e.g., [height] × [width] × [channel].\n","    x_train_org = x_train_org.reshape(x_train_org.shape[0], 32, 32, 3)\n","    x_test_org = x_test_org.reshape(x_test_org.shape[0], 32, 32, 3)\n","    return [x_train_org, x_test_org, y_train_org, y_test_org]\n","x_train_org, x_test_org, y_train_org, y_test_org = PreprocessDataset()\n","\n","CIFAR100_LABELS_LIST = [\n","    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n","    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n","    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n","    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n","    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n","    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n","    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n","    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n","    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n","    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n","    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n","    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n","    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n","    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n","    'worm'\n","]\n","\n","#Label chose:\n","label_choosen = ['bridge', 'castle', 'house', 'road', 'skyscraper','cloud', 'forest', 'mountain', 'plain','sea']\n","label_loc = []\n","for item in CIFAR100_LABELS_LIST:\n","  if item in label_choosen:\n","    print([CIFAR100_LABELS_LIST.index(item), item])\n","    label_loc.append(CIFAR100_LABELS_LIST.index(item))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[12, 'bridge']\n","[17, 'castle']\n","[23, 'cloud']\n","[33, 'forest']\n","[37, 'house']\n","[49, 'mountain']\n","[60, 'plain']\n","[68, 'road']\n","[71, 'sea']\n","[76, 'skyscraper']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0EQgum2RkNn8","colab_type":"text"},"source":["2) Combine testing and training data into one after you load the cifar100 data\n"]},{"cell_type":"code","metadata":{"id":"KEN0fDdNN6CZ","colab_type":"code","outputId":"e3a82af2-c500-4041-a380-5c46c058f646","executionInfo":{"status":"ok","timestamp":1556920773151,"user_tz":420,"elapsed":1213,"user":{"displayName":"Mengguo Yan","photoUrl":"","userId":"13426425900381067474"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["def concat(train,test):\n","  res = np.concatenate((train,test), axis = 0)\n","  print ('Shape:', res.shape)\n","  return res\n","x = concat(x_train_org,x_test_org)\n","y = concat(y_train_org,y_test_org)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Shape: (60000, 32, 32, 3)\n","Shape: (60000, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bH7VNmRgkR3V","colab_type":"text"},"source":["3) Filter to select images belonging to your assigned superclasses and make it ready to use"]},{"cell_type":"code","metadata":{"id":"rvYjOn_0JdCI","colab_type":"code","colab":{}},"source":["#set up subclass\n","def subclass(x, y):\n","  x_dict = {}\n","  y_dict = {}\n","  for i, item in enumerate(y):\n","    if np.argmax(item) in label_loc:\n","      if np.argmax(item) in x_dict.keys():\n","        x_dict[np.argmax(item)].append(x[i])\n","        y_dict[np.argmax(item)].append(y[i])\n","      else:\n","        x_dict[np.argmax(item)] = []\n","        x_dict[np.argmax(item)].append(x[i])\n","        y_dict[np.argmax(item)] = []\n","        y_dict[np.argmax(item)].append(y[i])\n","  return x_dict, y_dict\n","\n","x_dict,y_dict = subclass(x, y)\n","#x_train_dict, y_train_dict = subclass(x_train_org, y_train_org)\n","#x_test_dict, y_test_dict = subclass(x_test_org, y_test_org)\n","\n","# return np.array\n","binary_label_loc_0 = [12,17,37,68,76]\n","#binary_label_loc_test_0 = [37]\n","binary_label_loc_1 = [23,33,49,60,71]\n","#binary_label_loc_test_1 = [60]\n","\n","def subclass_np(x_dict, y_dict, labels):\n","  x = []\n","  y= []\n","  for key in labels:\n","    x.append(x_dict[key])\n","    y.append(y_dict[key])\n","  x = np.array(x)\n","  x = x.reshape(x.shape[0]*x.shape[1], 32,32,3)\n","  y = np.array(y)\n","  y =  y.reshape(y.shape[0]*y.shape[1], 100,1)\n","  return x,y\n","\n","x_0, y_0 = subclass_np(x_dict,y_dict,binary_label_loc_0)\n","#x_test_0,y_test_0 =    subclass_np(x_dict,y_dict,binary_label_loc_test_0)\n","x_1, y_1 = subclass_np(x_dict,y_dict, binary_label_loc_1)\n","#x_test_1, y_test_1 =   subclass_np(x_dict,y_dict, binary_label_loc_test_1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XrHYFSl1XkLU","colab_type":"code","outputId":"cb8d11d6-b088-4a46-c35a-0e2264a8e4f3","executionInfo":{"status":"ok","timestamp":1556920779683,"user_tz":420,"elapsed":358,"user":{"displayName":"Mengguo Yan","photoUrl":"","userId":"13426425900381067474"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#x_train = np.concatenate((x_train_0,x_train_1), axis = 0)\n","#x_test = np.concatenate((x_test_0,x_test_1),axis = 0)\n","#y_train_0 = np.zeros(x_train_0.shape[0])\n","#y_train_1 = np.ones(x_train_1.shape[0])\n","#y_test_0 = np.zeros(x_test_0.shape[0])\n","#y_test_1 = np.ones(x_test_1.shape[0])\n","\n","#x = np.concatenate((x_0,x_1), axis = 0)\n","\n","\n","y_0 = np.zeros(x_0.shape[0])\n","y_1 = np.ones(x_1.shape[0])\n","y = np.concatenate((y_0,y_1), axis = 0)\n","\n","#x = x.reshape(x.shape[0], 32*32*3)\n","x_0.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3000, 32, 32, 3)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"EKqWkzKRn08Z","colab_type":"text"},"source":["Image printing function definition "]},{"cell_type":"code","metadata":{"id":"_tZlJuu0nxOd","colab_type":"code","colab":{}},"source":["#Print img\n","def img_print(y_test, y_test_pred, model):\n","  index = [np.random.choice(range(len(y_test))) for i in range(6)]\n","  fig = plt.figure(figsize=(5, 5))\n","  fig.suptitle('Randomly selected pictures from %s'%model)\n","  n = 0\n","  for j in range(3):\n","    for k in range(2):\n","      i_inds = (j*2)+k\n","      i_data = index[n]\n","      ax = fig.add_subplot(2,3, i_inds+1)\n","      ax.imshow(x_test[i_data].reshape(32,32,3).astype(float), interpolation='nearest')\n","      label = \"ManMade\" if y_test_pred[i_data] == 0 else \"Natural\"\n","      if y_test[i_data] != y_test_pred[i_data]:\n","        ax.set_title(label, color = 'red')\n","      else:\n","        ax.set_title(label)  \n","      plt.axis('off')\n","      n+=1\n","  fig.tight_layout(rect=[0, 0.03, 1, 0.95])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r7Yps4yMkd7k","colab_type":"text"},"source":["### Data Selection\n","1. Based on previous milestone, the best performance include [Plain, House] and the worst include [Forest, Bridge]\n","2. So we choose three combinations of data for milestone3 as followed: <br>\n","      a. Test data: [House, Skyscraper] vs [Plain, Sea]  (37, 76) vs (60,71)   <br>\n","      b. Test data: [Bridge, Road] vs  [Forest, Mountain]  (12, 68) vs (33, 49)<br>\n","      c. Test data: [House, Road] vs [Forest, Plain]  (37, 68) vs  (33, 60)<br>\n"]},{"cell_type":"code","metadata":{"id":"6W5Ub-ovRBu_","colab_type":"code","colab":{}},"source":["def data_selection(label_test_0, label_test_1, x_dict):\n","  label_0 = [12,17,37,68,76]\n","  label_1 = [23,33,49,60,71]\n","  x_train_0 = []\n","  x_test_0 = []\n","  x_train_1 = []\n","  x_test_1 = []\n","  #print(label_test_0)\n","  for label in x_dict.keys():\n","    if label in label_0:\n","      if label in label_test_0:\n","          x_test_0.append(x_dict[label])\n","      else:\n","        x_train_0.append(x_dict[label])\n","    if label in label_1:\n","      if label in label_test_1:\n","        x_test_1.append(x_dict[label])\n","      else:\n","        x_train_1.append(x_dict[label])\n","  #print([len(x_train_0), len(x_test_0)])\n","  x_train = np.concatenate((x_train_0, x_train_1), axis = 0)\n","  x_train = x_train.reshape(6*600,32,32,3)\n","  x_test = np.concatenate((x_test_0, x_test_1), axis = 0)\n","  x_test = x_test.reshape(4*600, 32*32*3)\n","  return x_train, x_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UBK4EbhvYXf4","colab_type":"code","colab":{}},"source":["y_train = np.concatenate((np.zeros(1800), np.ones(1800)), axis = 0)\n","y_test = np.concatenate((np.zeros(1200), np.ones(1200)), axis = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lg4W4dScT6Ii","colab_type":"code","colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","def data_augmentation(x_0, y_0, augment_size=14400): \n","        \n","        train_size = x_0.shape[0]\n","        #train_1 = x_1.shape[0]\n","        image_generator = ImageDataGenerator(\n","            rescale = 1./255.,\n","            featurewise_center=True,\n","            featurewise_std_normalization=True,\n","            rotation_range=20,\n","            zoom_range = 0.05, \n","            width_shift_range=0.07,\n","            height_shift_range=0.07,\n","            horizontal_flip=True,\n","            vertical_flip=False, \n","            data_format=\"channels_last\",\n","            zca_whitening=True)\n","        # fit data for zca whitening\n","        image_generator.fit(x_0, augment=True)\n","        # get transformed images\n","        randidx = np.random.randint(train_size, size=augment_size)\n","        x_augmented = x_0[randidx].copy()\n","        y_augmented = y_0[randidx].copy()\n","        x_augmented = image_generator.flow(x_augmented, np.zeros(augment_size),\n","                                    batch_size=augment_size, shuffle=False).next()[0]\n","        # append augmented data to trainset\n","        x_0 = np.concatenate((x_0, x_augmented))\n","        y_0 = np.concatenate((y_0, y_augmented))\n","        #train_size = x_0.shape[0]\n","        #test_size = x_test.shape[0]\n","        \n","        return x_0, y_0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z5H39ulWT9GE","colab_type":"code","colab":{}},"source":["import cv2\n","def color_pix_r(x):\n","    percentage = []\n","    for img in x:\n","    #color = set()\n","        pix = img.reshape(1024,3)\n","        clr = [str(p) for p in pix]\n","        color = list(set(clr))\n","        per = len(color)/1024\n","        percentage.append(per)\n","    percentage = np.array(percentage)\n","    return percentage\n","\n","def color_sat(x):\n","    sat_array = []\n","    for array in x:\n","        array = array.reshape(1024,3)\n","        sat = np.array([max(abs(pix[0]-pix[1]),abs(pix[0]-pix[2]),abs(pix[2]-pix[1])) for pix in array])\n","        sat_array.append(np.mean(sat))\n","    return np.array(sat_array)\n","#----------------------------------------------------\n","def geo_trans(image):\n","    img = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY).astype(float)\n","    rows,cols = img.shape\n","    M = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)\n","    dst = cv2.warpAffine(img,M,(cols,rows))\n","    return dst.flatten()\n","\n","def transform_geo(x_train, x_test):\n","    x_train_transformed = []\n","    for item in x_train:\n","        x_train_transformed.append(geo_trans(item))\n","    x_test_transformed = []\n","    for item in x_test:\n","        x_test_transformed.append(geo_trans(item))\n","    return np.array(x_train_transformed), np.array(x_test_transformed)\n","#----------------------------------------------------\n","def img_gradient(image):\n","    img = cv2.cvtColor(image.reshape(32,32,3), cv2.COLOR_BGR2GRAY).astype(float)\n","    laplacian = cv2.Laplacian(img,cv2.CV_64F)\n","    sobelx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)\n","    sobely = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)\n","    return np.array([laplacian, sobelx, sobely]).flatten()\n","\n","def transform_gradient(x_train, x_test):\n","    x_train_transformed = []\n","    for i, item in enumerate(x_train):\n","        x_train_transformed.append(img_gradient(item))\n","        x_test_transformed = []\n","    for i, item in enumerate(x_test):\n","        x_test_transformed.append(img_gradient(item))\n","    return np.array(x_train_transformed), np.array(x_test_transformed)\n","#----------------------------------------------------\n","def fd_histogram(image, mask=None):\n","    # convert the image to HSV color-space\n","    image = cv2.cvtColor(image.reshape(32,32,3), cv2.COLOR_BGR2HSV)\n","    # compute the color histogram\n","    hist  = cv2.calcHist([image], [0, 1, 2], None, [32, 32, 32], [0, 256, 0, 256, 0, 256])\n","    # normalize the histogram\n","    cv2.normalize(hist, hist)\n","    return hist.flatten()\n","  \n","def transform_hist(x_train, x_test):\n","    x_train_transformed = []\n","    for item in x_train:\n","        x_train_transformed.append(fd_histogram(item))\n","        x_test_transformed = []\n","    for item in x_test:\n","        x_test_transformed.append(fd_histogram(item))\n","    return np.array(x_train_transformed), np.array(x_test_transformed)\n","#-------------------------------------------------------\n","def to_gray(image):\n","    image = cv2.cvtColor(image.reshape(32,32,3), cv2.COLOR_BGR2GRAY).astype(float)\n","    return image.reshape(1,image.shape[0]*image.shape[1])[0]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbogX1BmUCqV","colab_type":"code","colab":{}},"source":["binary_label_loc_0 = [12,17,37,68,76]\n","binary_label_loc_1 = [23,33,49,60,71]\n","\n","C_0 = []\n","for i, first in enumerate(binary_label_loc_0):\n","    for j, second in enumerate(binary_label_loc_0[i+1:]):\n","        C_0.append([first, second])\n","C_1 = []\n","for i, first in enumerate(binary_label_loc_1):\n","    for j, second in enumerate(binary_label_loc_1[i+1:]):\n","        C_1.append([first, second])\n","        \n","C_0_name = [[CIFAR100_LABELS_LIST[x[0]],CIFAR100_LABELS_LIST[x[1]]] for x in C_0]\n","C_1_name = [[CIFAR100_LABELS_LIST[x[0]],CIFAR100_LABELS_LIST[x[1]]] for x in C_1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_EfJpDE9eZE8","colab_type":"text"},"source":["###Logistic Regression"]},{"cell_type":"code","metadata":{"id":"jYKmbV8FebuN","colab_type":"code","colab":{}},"source":["result = []\n","for first_item in C_0:\n","    for second_item in C_1:\n","        x_train, x_test = data_selection(first_item,second_item,x_dict )\n","        x_augment, y_augment = data_augmentation(x_train,y_train, augment_size=9600)\n","        x_aug_gray = np.array([to_gray(x) for x in x_augment])\n","        x_test_gray = np.array([to_gray(x) for x in x_test])\n","\n","        x_aug_sat = color_sat(x_augment)\n","        x_aug_sat_test = color_sat(x_test)\n","        x_aug_sat = x_aug_sat.reshape(-1,1)\n","        x_aug_sat_test = x_aug_sat_test.reshape(-1,1)\n","\n","\n","        x_aug_grad, x_test_grad =  transform_gradient(x_augment, x_test)\n","        \n","        x_augment = x_augment.reshape(x_augment.shape[0], 32*32*3)\n","        x_aug_combine= np.concatenate((x_aug_gray, x_aug_grad, x_aug_sat), axis = 1)\n","\n","        x_test = x_test.reshape(x_test.shape[0], 32*32*3)\n","        x_test_combine = np.concatenate((x_test_gray, x_test_grad, x_aug_sat_test), axis = 1)\n","        \n","        pca = PCA(0.9)\n","        pca.fit(x_aug_combine)\n","        x_train_aug = pca.transform(x_aug_combine)\n","        x_test_aug = pca.transform(x_test_combine)\n","        ### Modify this part ######\n","        lg = LogisticRegression()\n","        lg.fit(x_train_aug, y_augment)\n","        score = lg.score(x_test_aug, y_test)\n","        #print(score)\n","        result.append(score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sg8kHyBCemhX","colab_type":"code","colab":{}},"source":["result_reshape = np.array(result).reshape(10,10)\n","sbn.heatmap(result_reshape,cmap=\"Blues\")\n","plt.xticks(np.arange(10), C_0_name, rotation=90)\n","plt.yticks(np.arange(10), C_1_name, rotation = 45)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNXeciO3d1jQ","colab_type":"text"},"source":["### GradientBoosting"]},{"cell_type":"code","metadata":{"id":"p2I9i9VbUGYn","colab_type":"code","colab":{}},"source":["result = []\n","for first_item in C_0:\n","    for second_item in C_1:\n","        x_train, x_test = data_selection(first_item,second_item,x_dict )\n","        x_augment, y_augment = data_augmentation(x_train,y_train, augment_size=9600)\n","        x_aug_gray = np.array([to_gray(x) for x in x_augment])\n","        x_test_gray = np.array([to_gray(x) for x in x_test])\n","\n","        x_aug_sat = color_sat(x_augment)\n","        x_aug_sat_test = color_sat(x_test)\n","        x_aug_sat = x_aug_sat.reshape(-1,1)\n","        x_aug_sat_test = x_aug_sat_test.reshape(-1,1)\n","\n","\n","        x_aug_grad, x_test_grad =  transform_gradient(x_augment, x_test)\n","        \n","        x_augment = x_augment.reshape(x_augment.shape[0], 32*32*3)\n","        x_aug_combine= np.concatenate((x_aug_gray, x_aug_grad, x_aug_sat), axis = 1)\n","\n","        x_test = x_test.reshape(x_test.shape[0], 32*32*3)\n","        x_test_combine = np.concatenate((x_test_gray, x_test_grad, x_aug_sat_test), axis = 1)\n","        \n","        pca = PCA(0.9)\n","        pca.fit(x_aug_combine)\n","        x_train_aug = pca.transform(x_aug_combine)\n","        x_test_aug = pca.transform(x_test_combine)\n","        ### Modify this part ######\n","        clf = GradientBoostingClassifier()\n","        clf.fit(x_train_aug, y_augment)\n","        score = clf.score(x_test_aug, y_test)\n","        #print(score)\n","        result.append(score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eySKC9tmUhhy","colab_type":"code","colab":{}},"source":["result_reshape = np.array(result).reshape(10,10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KRJ1jYWT95za","colab_type":"code","colab":{}},"source":["sbn.heatmap(result_reshape,cmap=\"Blues\")\n","plt.xticks(np.arange(10), C_0_name, rotation=90)\n","plt.yticks(np.arange(10), C_1_name, rotation = 45)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LiJZHcDNeEs6","colab_type":"text"},"source":["### Random Forest"]},{"cell_type":"code","metadata":{"id":"YfTiGsIBeGx2","colab_type":"code","colab":{}},"source":["result = []\n","for first_item in C_0:\n","    for second_item in C_1:\n","        x_train, x_test = data_selection(first_item,second_item,x_dict )\n","        x_augment, y_augment = data_augmentation(x_train,y_train, augment_size=9600)\n","        x_aug_gray = np.array([to_gray(x) for x in x_augment])\n","        x_test_gray = np.array([to_gray(x) for x in x_test])\n","\n","        x_aug_sat = color_sat(x_augment)\n","        x_aug_sat_test = color_sat(x_test)\n","        x_aug_sat = x_aug_sat.reshape(-1,1)\n","        x_aug_sat_test = x_aug_sat_test.reshape(-1,1)\n","\n","\n","        x_aug_grad, x_test_grad =  transform_gradient(x_augment, x_test)\n","        \n","        x_augment = x_augment.reshape(x_augment.shape[0], 32*32*3)\n","        x_aug_combine= np.concatenate((x_aug_gray, x_aug_grad, x_aug_sat), axis = 1)\n","\n","        x_test = x_test.reshape(x_test.shape[0], 32*32*3)\n","        x_test_combine = np.concatenate((x_test_gray, x_test_grad, x_aug_sat_test), axis = 1)\n","        \n","        pca = PCA(0.9)\n","        pca.fit(x_aug_combine)\n","        x_train_aug = pca.transform(x_aug_combine)\n","        x_test_aug = pca.transform(x_test_combine)\n","        ### Modify this part ######\n","        clf = GradientBoostingClassifier()\n","        clf.fit(x_train_aug, y_augment)\n","        score = clf.score(x_test_aug, y_test)\n","        #print(score)\n","        result.append(score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uf2hfk95eKo4","colab_type":"code","colab":{}},"source":["result_reshape = np.array(result).reshape(10,10)\n","sbn.heatmap(result_reshape,cmap=\"Blues\")\n","plt.xticks(np.arange(10), C_0_name, rotation=90)\n","plt.yticks(np.arange(10), C_1_name, rotation = 45)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"800iEOzZxjig","colab_type":"text"},"source":["### Dense Net\n"]},{"cell_type":"code","metadata":{"id":"ezW-HT-OxlS1","colab_type":"code","colab":{}},"source":["from keras.datasets import cifar100\n","from __future__ import print_function\n","\n","import keras.callbacks as cb\n","from keras.layers import Activation, Dense, Dropout, Flatten\n","from keras.layers.convolutional import Conv2D\n","from keras.layers.pooling import MaxPooling2D\n","from keras.models import Sequential, Model\n","from keras.optimizers import SGD\n","from keras.regularizers import l1, l2\n","from keras.utils import np_utils\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","import numpy as np\n","import time\n","\n","from sklearn.metrics import classification_report\n","from numpy.random import seed\n","seed(1)\n","from tensorflow import set_random_seed\n","set_random_seed(2)\n","\n","num_predictions = 36\n","from keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sn\n","import pandas as pd\n","from keras.models import Model\n","from keras.layers import Dense, GlobalAveragePooling2D\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from keras.applications.densenet import DenseNet201,preprocess_input\n","import itertools\n","\n","CIFAR100_LABELS_LIST = [\n","    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', \n","    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', \n","    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', \n","    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', \n","    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', \n","    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n","    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n","    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n","    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n","    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n","    'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n","    'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n","    'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n","    'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman',\n","    'worm'\n","]\n","\n","CIFAR100_LABELS_LIST.sort()\n","\n","\n","batch_size=64\n","datagen = ImageDataGenerator(\n","\tfeaturewise_center=False, \n","\tsamplewise_center=False,  \n","\tfeaturewise_std_normalization=False,\n","\tsamplewise_std_normalization=False, \n","\tzca_whitening=False, \n","\trotation_range=0, \n","\twidth_shift_range=0, \n","\theight_shift_range=0,\n","\thorizontal_flip=False, \n","\tvertical_flip=False) \n","\n","\n","# https://keras.io/applications/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HYrmi4GkxpGB","colab_type":"code","colab":{}},"source":["def PreprocessDataset():\n","    (x_train_org, y_train_org), (x_test_org, y_test_org) = cifar100.load_data(label_mode='fine')\n","    x_train_org = x_train_org.astype('float32')\n","    x_test_org = x_test_org.astype('float32')\n","    # Normalize value to [0, 1]\n","    #x_train_org /= 255\n","    #x_test_org /= 255\n","    # Transform lables to one-hot\n","    y_train_org = np_utils.to_categorical(y_train_org, 100)\n","    y_test_org = np_utils.to_categorical(y_test_org, 100)\n","    # Reshape: here x_train is re-shaped to [channel] × [width] × [height]\n","    # In other environment, the orders could be different; e.g., [height] × [width] × [channel].\n","    x_train_org = x_train_org.reshape(x_train_org.shape[0], 32, 32, 3)\n","    x_test_org = x_test_org.reshape(x_test_org.shape[0], 32, 32, 3)\n","    return [x_train_org, x_test_org, y_train_org, y_test_org]\n","\n","x_train_org, x_test_org, y_train_org, y_test_org = PreprocessDataset()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U11oQ1H0xpsx","colab_type":"code","colab":{}},"source":["def subclass(x, y,loc):\n","  x_dict = {}\n","  y_dict = {}\n","  for i, item in enumerate(y):\n","    if np.argmax(item) in loc:\n","      if np.argmax(item) in x_dict.keys():\n","        x_dict[np.argmax(item)].append(x[i])\n","        y_dict[np.argmax(item)].append(y[i])\n","      else:\n","        x_dict[np.argmax(item)] = []\n","        x_dict[np.argmax(item)].append(x[i])\n","        y_dict[np.argmax(item)] = []\n","        y_dict[np.argmax(item)].append(y[i])\n","  return x_dict, y_dict\n","\n","\n","# return np.array\n","def subclass_np(x_dict, y_dict):\n","  x = []\n","  y= []\n","  for key in x_dict.keys():\n","    x.append(x_dict[key])\n","    y.append(y_dict[key])\n","  x = np.array(x,dtype=np.float32)\n","  x = x.reshape(x.shape[0]*x.shape[1], 32,32,3)\n","  y = np.array(y)\n","  y =  y.reshape(y.shape[0]*y.shape[1], 100,1)\n","  return x,y\n","\n","binary_label_loc_0 = [12,17,37,68,76]\n","binary_label_0 = ['bridge', 'castle', 'house', 'road', 'skyscraper']\n","\n","def y_binary(y_list):\n","  y_bi = []\n","  for item in y_list:\n","    if np.argmax(item) in binary_label_loc_0:\n","      y_bi.append(0)\n","    else:\n","      y_bi.append(1)\n","  return np.array(y_bi)\n","\n","def cifar_grid_c(X,Y,inds,n_col, predictions=None):\n","  if predictions is not None:\n","    if Y.shape != predictions.shape:\n","      print(\"Predictions must equal Y in length!\")\n","      print(Y.shape)\n","      print(predictions.shape)\n","      return(None)\n","  N = len(inds)\n","  n_row = int(np.ceil(1.0*N/n_col))\n","  fig, axes = plt.subplots(n_row,n_col,figsize=(10,30))\n","  \n","#  clabels = labels[\"label_names\"]\n","  clabels = [\"man-made\",\"nature\"]\n","  for j in range(n_row):\n","    for k in range(n_col):\n","      i_inds = (j*n_col)+k\n","      i_data = inds[i_inds]\n","      \n","     # axes[j][k].set_axis_off()\n","      if i_inds < N:\n","        rgb = X[[i_data,...]]\n","        img = rgb.astype(np.uint8)\n","        #do not need reshap here. img = rgb.reshape(3,32,32).transpose([1, 2, 0])\n","        axes[j][k].imshow(img)\n","        label = clabels[np.argmax(Y[i_data,...])]\n","        axes[j][k].set_title(label)\n","        if predictions is not None:\n","          pred = clabels[np.argmax(predictions[i_data,...])]\n","          if label != pred:\n","            label += \" n\"\n","            axes[j][k].set_title(pred, color='red')            \n","  \n","  fig.set_tight_layout(True)\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmLClfhexrsA","colab_type":"code","colab":{}},"source":["import gc \n","# will be total of 100 combos\n","counter = 0\n","for ab in itertools.combinations(['bridge', 'castle', 'road', 'skyscraper','house'], 2):\n","  for cd in itertools.combinations(['plain','cloud', 'forest', 'mountain','sea'], 2):\n","    label_choosen_train = ['bridge', 'castle', 'road', 'skyscraper','house', 'plain', 'cloud', 'forest', 'mountain','sea']\n","    a,b = ab\n","    c,d = cd\n","    label_choosen_test = [a,b,c,d]\n","    print(\"#######################################################\")\n","    print(\"#######################################################\")\n","    print(\"#######################################################\")\n","    print(\"####### combo:%d, using %s,%s,%s,%s as testing ########\" %(counter,a,b,c,d))\n","    print(\"#######################################################\")\n","    print(\"#######################################################\")\n","    print(\"#######################################################\")\n","    gc.collect()\n","    if counter <97:\n","      counter += 1\n","      continue\n","    counter += 1\n","    for i in label_choosen_test:\n","      label_choosen_train.remove(i)\n","\n","    label_loc_train = []\n","    label_loc_test = []\n","    for item in CIFAR100_LABELS_LIST:\n","      if item in label_choosen_train:\n","        print([CIFAR100_LABELS_LIST.index(item), item])\n","        label_loc_train.append(CIFAR100_LABELS_LIST.index(item))\n","      if item in label_choosen_test:\n","        print([CIFAR100_LABELS_LIST.index(item), item])\n","        label_loc_test.append(CIFAR100_LABELS_LIST.index(item))\n","\n","    # sub_class selection -- return dictionary (for later milestone 2 usage)\n","\n","    x_train_dict, y_train_dict = subclass(x_train_org, y_train_org, label_loc_train)\n","    x_test_dict, y_test_dict = subclass(x_test_org, y_test_org, label_loc_test)\n","\n","    x_train, y_train_list = subclass_np(x_train_dict, y_train_dict)\n","    x_test,  y_test_list = subclass_np(x_test_dict, y_test_dict)\n","    y_train = y_binary(y_train_list)\n","    y_test = y_binary(y_test_list)\n","    print([x_train.shape, y_train.shape, x_test.shape, y_test.shape])\n","    print([np.unique(y_train),np.bincount(y_train)])\n","    n = 150\n","    some_key = list(x_train_dict.keys())[0]\n","    for i in range(5):\n","      n = n+1\n","      plt.subplot(n)\n","      plt.imshow(x_train_dict[some_key][i])\n","      plt.imshow(x_train_dict[some_key][i].astype(np.uint8))\n","\n","    base_model = DenseNet201(include_top=False, weights='imagenet', input_shape=(32,32,3))\n","    densenet_input_train = preprocess_input(x_train)\n","    densenet_input_test = preprocess_input(x_test)\n","    x = base_model.output\n","    x = GlobalAveragePooling2D()(x)\n","    # let's add a fully-connected layer\n","    x = Dense(1024, activation='relu')(x)\n","    # and a logistic layer -- let's say we have 200 classes\n","    predictions = Dense(2, activation='softmax')(x)\n","    # this is the model we will train\n","    model = Model(inputs=base_model.input, outputs=predictions)\n","    for layer in base_model.layers:\n","        layer.trainable = False\n","    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n","    model.fit(densenet_input_train, to_categorical(y_train, num_classes=2), batch_size=64, epochs=3,\n","              validation_data=(densenet_input_test, to_categorical(y_test, num_classes=2)),\n","              verbose=2, shuffle=True)\n","\n","\n","    y_tr_predict = model.predict(densenet_input_train)\n","    y_te_predict = model.predict(densenet_input_test)\n","    y_predict = np.concatenate((y_tr_predict,y_te_predict))\n","    y_ = np.concatenate((y_train,y_test))\n","    print(\"combined classification report\")\n","    y_predict = np.argmax(y_predict,1)\n","    print(classification_report(y_, y_predict))\n","    labels = np.unique(y_)\n","    cm = confusion_matrix(y_, y_predict,labels)\n","    plt.figure(figsize = (6,6))\n","    df_cm = pd.DataFrame(cm, index = [i for i in labels],\n","                      columns = [i for i in labels])\n","    ax=sn.heatmap(df_cm, annot=True)\n","    #print out randomly selected images(black/red labl-->correct/wrong prediction )\n","\n","\n","    x_ = np.concatenate((x_train,x_test))\n","    datagen.fit(x_train)\n","    predict_gen = model.predict_generator(datagen.flow(densenet_input_test, to_categorical(y_test, 2),\n","        batch_size=batch_size, shuffle=False),\n","        steps=(x_test.shape[0] // batch_size)+1, workers=4)\n","\n","    indices = [np.random.choice(range(len(x_test))) \n","               for i in range(num_predictions)]\n","    x_test,  y_test_list = subclass_np(x_test_dict, y_test_dict)\n","    cifar_grid_c(x_test,to_categorical(y_test, 2), indices,6, predictions=to_categorical(np.argmax(predict_gen,1),2))"],"execution_count":0,"outputs":[]}]}